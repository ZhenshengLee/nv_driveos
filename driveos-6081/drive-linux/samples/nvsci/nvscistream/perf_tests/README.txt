NvSciStream Perf Test App - README

Copyright (c) 2021-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.

NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
property and proprietary rights in and to this material, related
documentation and any modifications thereto. Any use, reproduction,
disclosure or distribution of this material and related documentation
without an express license agreement from NVIDIA CORPORATION or
its affiliates is strictly prohibited.

---
# test_nvscistream_perf - NvSciStream Perf Test App

## Description

This directory contains an NvSciStream performance test application which
uses NvSciStream, NvSciBuf, NvSciSync and NvSciIpc to transmit buffers
between a CPU producer and CPU consumer(s). This test focus on NvSciStream
performance, which doesn't use CUDA, NvMedia or other hardware engines.
To make it easier to measure the packet-delivery latency for each payload,
the stream uses FIFO mode.

This test app is for performance testing purpose, not as an example to
show how to use NvSciStream APIs. It simplifies some setup steps, like
skip querying element type, synchronization object and packet status. It
also adds unnecessary steps to create and exchange the NvSciSync objects
for the CPU clients, as we want to include the latency to transport the
synchronization objects and fences. We have a sample application to show
how to create a stream with NvSciStream APIs, nvscistream_event_sample.

This test utilizes the NvPlayFair library to record timestamps, set the
rate limit, save raw latency data and calculate the latency statistics,
like min, max and mean value, on different platforms and OSs.

The test app supports a variety of test cases:

* Single-process test: Producer and consumer(s) are in the same process.
  Producer, pool and each consumer has its own thread to handle events.
* Inter-process test: Producer and each consumer are in different processes.
  Pool waits for and handles events on a separate thread.
* Inter-chip (C2C) test: Producer and each consumer are on different SoCs.
  Pool waits for and handles events on a separate thread.

* Unicast-stream test: Connect one producer to one consumer in one stream.
* Multicast-stream test: Connect one producer to multiple consumers in one
  stream.

The test can set different stream configurations:

* Number of packets allocated in pool.
* Number of payloads transmitted between producer and consumer(s).
* Buffer size for each element.
* Number of synchronization objects used by each endpoint.
* Frame rate, frequency of the payloads presented by the producer.
* Memory type, vidmem or sysmem.

The test measures several performance KPIs:

Latency for each process:

* Total init time: The duration for all setup steps before streaming,
  including the NvSciBuf, NvSciSync and NvSciIpc resource initialization.
  Some processes may take long initialization time as they need to wait
  for other process to start and connect.
* Setup time: The duration between stream connected and setup completed.
* Streaming time: The duration between setup completed and stream
  disconnected.

Latency for each payload:

* 1) Producer packet wait:
  The duration that the producer waits for the packet-ready event. It helps
  to decide the number of packets to be allocated in the pool to unblock the
  producer process.
* 2) Producer packet present:
  The packet-delivery latency from the time the producer presents the packet
  to the time the consumer acquires the packet. The producer records the
  timestamp and writes it into the packet, and then sends the packet to the
  consumer(s). The consumer records the timestamp after it acquires the packet
  before waiting on the fences generated by the producer.
* 3) Producer packet present + engine write done:
  The packet-delivery latency from the time the producer presents the packet
  to the time the consumer acquires the packet and waits on pre-fences.
  Similar to 2), but it also includes the duration to wait for producer write
  complete.
  In non-C2C use case, it's unnecessary to set the fences, as the producer
  finishes CPU write before sending the packet. But this test still generates
  fences and sends them to the other end to include the latency of the fences
  transport, fence query and fence wait. It should not block the consumer to
  wait on the expired fences.
  To measure the accurate end-to-end packet-delivery latency, it's better to
  use one packet in pool. If there're more than one packet, each packet may
  need to wait for the processing of other packets.
* 4) Consumer packet wait:
  The duration that the consumer waits for the packet-ready event.
* 5) Consumer packet release:
  The packet-delivery latency from the time the consumer releases the packet
  to the time the producer gets the packet before waiting on the fences
  generated by the producer.
  Similar to 2), but in the reverse direction.
  In non-C2C use case, we don't measure the packet-release latency. Because
  the producer and consumer(s) use different packet pools. After triggering
  the C2C copy (without waiting for copy done), the packet in the upstream
  pool will be returned to the producer, and a packet in the downstream (c2c)
  pool(s) will be sent to the consumer(s). In this case, the producer may
  receive the packet earlier than the consumer(s).

Specific to inter-chip (C2C) use case:

* 6) Producer packet present + C2C copy done:
  Similar to 3), the consumer(s) wait for the C2C copy-done fences from the
  producer. In non-C2C case, producer and consumer(s) share the buffer, so
  it doesn't need to copy buffer data. In C2C case, the buffer could not be
  shared, which requires data copy. The fences from the producer may not be
  expired. It takes the consumer longer time than that in 3) to wait on the
  fences.
* 7) PCIe bandwidth:
  The total buffer size transmitted by NvSciStream per second.
  (PCIe bandwidth = num_of_payloads * buffer_size_per_packet / streaming_time)

## Build the application

The NvSciStream perf test includes source code and a Makefile.
Navigate to the test application directory to build the perf test:

    make clean
    make

## Test command

See the help() message in the main.c for different tests options. Here're some
examples of how to run the perf test:
(NOTE: Inter-process/Inter-chip use cases must be run with sudo.)

1.  Measure latency for single-process unicast stream:

    ./test_nvscistream_perf -l

2.  Measure latency for single-process unicast stream with three packets
    in pool:

    ./test_nvscistream_perf -l -k 3

3.  Measure latency for single-process multicast stream with two consumers:

    ./test_nvscistream_perf -n 2 -l

4.  Measure latency for single-process multicast stream with two consumers
    and three packets in pool:

    ./test_nvscistream_perf -n 2 -l -k 3

5.  Measure latency for inter-process unicast stream:

    ./test_nvscistream_perf -p -l &
    ./test_nvscistream_perf -c 0 -l

6.  Measure latency for inter-process unicast stream with three packets
    in pool:

    ./test_nvscistream_perf -p -l -k 3 &
    ./test_nvscistream_perf -c 0 -l

7.  Measure latency for inter-process multicast stream with two consumers:

    ./test_nvscistream_perf -p -n 2 -l &
    ./test_nvscistream_perf -c 0 -l &
    ./test_nvscistream_perf -c 1 -l

8.  Measure latency for inter-process unicast stream with a fixed
    producer-present rate at 100 fps, which transmits 10,000 payloads:

    ./test_nvscistream_perf -p -f 10000 -l -r 100 &
    ./test_nvscistream_perf -c 0 -f 10000 -l

9.  Measure latency and save raw latency data in nvscistream_*.csv file
    for inter-process unicast stream, which transmits 10 payloads:

    ./test_nvscistream_perf -p -f 10 -l -v &
    ./test_nvscistream_perf -c 0 -f 10 -l -v

10. Measure latency and check whether the test result meets the input
    average latency target (110 us) and the 99.99 percentile latency target
    (520 us) with 5% tolerance range for the inter-process unicast stream,
    which transmits 1000 payloads:

    ./test_nvscistream_perf -p -f 1000 -l &
    ./test_nvscistream_perf -c 0 -f 1000 -l -a 110 -m 520

11. Measure latency for the inter-chip unicast stream with 12.5 MB buffer
   size per packet, which transmits 10,000 frames. The two commands are run
   on different SoCs:

    ./test_nvscistream_perf -P 0 nvscic2c_pcie_s0_c5_1 -l -b 12.5 -f 10000
    ./test_nvscistream_perf -C 0 nvscic2c_pcie_s0_c6_1 -l -b 12.5 -f 10000

12. Measure PCIe bandwidth for the inter-chip unicast stream with three
    packets in each pool and 12.5 MB buffer size per packet, which transmits
    10,000 frames. The two commands are run on different SoCs:

    ./test_nvscistream_perf -P 0 nvscic2c_pcie_s0_c5_1 -l -b 12.5 -k 3 -f 10000
    ./test_nvscistream_perf -C 0 nvscic2c_pcie_s0_c6_1 -l -b 12.5 -k 3 -f 10000

13. Measure PCIe bandwidth for the inter-chip unicast stream with three
    packets in each pool and 12.5 MB buffer size per packet using vidmem, which
    transmits 10,000 frames. (NOTE: only set vidmem if there's dGPU.)
    GPU UUID is retrieved from 'nvidia-smi -L' command.
    For example: If output of 'nvidia-smi -L' is UUID: GPU-aaaaaaaa-1111-bbbb-2222-ccccddddeee
    then GPU_UUID=aaaaaaaa1111bbbb2222ccccddddeee.
    The two commands are run on different SoCs:

    ./test_nvscistream_perf -P 0 nvscic2c_pcie_s0_c5_1 -b 12.5 -t 1 -k 3 -f 10000 -u <GPU_UUID>
    ./test_nvscistream_perf -C 0 nvscic2c_pcie_s0_c6_1 -b 12.5 -t 1 -k 3 -f 10000 -u <GPU_UUID>

13. Run inter-process unicast stream with a fixed producer-present rate at 100
    fps, without using synchronization objects or measuring latency (no buffer
    read/write). It may be used when measuring CPU and memory utilization:

    ./test_nvscistream_perf -p -s 0 -r 100 &
    ./test_nvscistream_perf -c 0 -s 0
